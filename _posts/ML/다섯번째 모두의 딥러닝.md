

## Multinomial이란?

지난 시간에 배웠던 Simple logistic regression은 binomial 문제로 예측하는 결과 값이 True/False 형태로 이분법적인 형태를 나타냈다. Multinomial은 binomial의 확장 버전으로 예측하는 결과 값이 여러개의 클래스가 될 수 있을 때 사용한다. 예를 들면 쉽게 이해가 되는 데, 대학교 강의 성적 결과를 생각해보자. 

만일 어떤 특정 강의를 듣고 성적 결과를 예측하는 모델을 만든 다고 가정을 할 때에 이 강의가 Pass/Fail 과목이라고 생각을 해보자. 그렇다면 우리가 예측하고 싶어하는 결과값은 이분법적으로 binomial classification을 적용해야한다.

만일 예측하고자 하는 강의가 Grade로 성적을 매긴다면? 그렇다면 우리가 예측하고 싶어하는 결과는 A, B, C, D, F와 같이 여러개의 결과가 있을 것이다. 이를 우리는 multinomial이라 부르며 이를 분류하는 것을 multinomial classification이라고 한다. 이번 시간에는 multinomial classification, 그 중에서도 가장 유명한 Softmax regression에 대해서 배울 것이다.

## Soft max regresssion

### Multinomial classification

Multinomial classification은 Binomial classification의 확장판으로 생각할 수 있다. 간단하게 예측하고자 하는 성적을 A, B, C 이렇게 세가지 경우가 있다고 생각해보자. 이런 경우에 binomial 관점에서, A인지 아닌지, B인지 아닌지, C인지 아닌지 이런 식으로 구분을 세번 할 수 가 있다. 강의에 나오는 예시 그래프는 다음과 같다.

![image-20220704134501026](../../assets/images/posts/다섯번째 모두의 딥러닝//image-20220704134501026.png)

각 예측하고자 하는 결과값을 각각 우리가 배웠던 binomial로 logistic regression을 사용해서 다음과 같이 해결하는 것이다.

![image-20220704134636150](../../assets/images/posts/다섯번째 모두의 딥러닝//image-20220704134636150.png)

각 결과값의 vector을 matrix로 표현하면 다음과 같다.

![image-20220704134716322](../../assets/images/posts/다섯번째 모두의 딥러닝//image-20220704134716322.png)

![image-20220704134803746](../../assets/images/posts/다섯번째 모두의 딥러닝//image-20220704134803746.png)

각각의 결과값 $Y_A, Y_B, Y_C$을 Sigmoid function을 거쳐 [0,1]의 범위로 표현 할 수 있다.  
예를 들어서, $Y_A=2.0, Y_B=1.0, Y_C=0.1$로 결과값이 나왔다고 하자. 이를 Sigmoid 함수를 적용해 각각 0.87, 0.35, 0.05의 확률 값을 갖게 된다면, 그렇다면 우리는 $Y_A$ 예측 값이 가장 큰 값이기 때문에, 해당 입력 값에 대하여 분류는 A라고 예상할 수 있게 된다.

그런데 여기서 각 요소들의 전체 확률의 합계가 1이 되도록 하여 전체 결과 선택지에 걸친 확률로 표현할 수 없을까? 예를 들어서 $Y_A$의 확률이 0.7이고, $Y_B$가 0.25, $Y_C$가 0.05로 세 확률의 총합이 1이 되도록 예측 값을 얻을 수 없을까? 이렇게 표현한다면, 확률의 총합이 1이므로, 어떤 분류에 속할 확률이 가장 높을지 쉽게 인지할 수 있다. 이럴 때 사용할 수 있는 것이 **Softmax function**이다.







### Softmax Function

정리하자면, 소프트맥스는 세 개 이상으로 분류해야하는 **multinomial classification**에서 사용하는 함수이며, 분류할 결과값가 n개라고 할 때, n차원의 벡터를 입력받아서, 이를 각 **결과값에 속할 확률을 추정**한다.

#### Soft max function formula

$$
Y_k = \frac{e^{a_k}}{\sum_{i=1}^{n}e^{a_i}}
$$

- $Y_k$는 각 결과(출력층, 클래스)에 대한 확률

- $n$은 결과값의 개수(출력층의 뉴런 수, 총 클래스의 수), $k$는 k번째 클래스

- 만일, 총 결과의 개수가 3개라고 한다면 다음과 같다.
  $$
  softmax(z)=[\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_2}},
              \frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_2}},
              \frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_2}},]
              =[p_1,p_2,p_3]
  $$
  위의 식을 보면, softmax function은 "k번째 결과일 확률 / 전체 확률"로 간단하다는 것을 알 수 있다.



다시 위에서 예를 들었던 것으로 돌아오게 된다면 결과값으로 나오게 된 $Y_A=2.0, Y_B=1.0, Y_C=0.1$를 softmax를 통해 다음과 같이 표현할 수 있게 된다.

![image-20220704145259848](../../assets/images/posts/다섯번째 모두의 딥러닝//image-20220704145259848.png)



실제로는 하나의 답을 추출해내기 위해서 'One-hot encoding'기법으로 하나의 데이터만 1.0값으로 세팅하고 이외의 값들은 0.0으로 처리한다.

![image-20220705131408633](../../assets/images/posts/다섯번째 모두의 딥러닝//image-20220705131408633.png)

여기까지 hypothesis 설정 단계이다. 이렇게 하면 우리의 모델, 우리의 hypothesis를 만들어 낸 것인데, 문제는 이에 대한 Cost function은 어떻게 설계하는 가이다. 



### Cross Entropy

#### Entropy

Entropy는 불확실성의 척도를 말한다. 즉, 엔트로피가 높다는 것은 정보가 많고, 확률이 낮다는 것을 의미하고, 엔트로피가 낮다는 것은 정보가 적은 대신, 확률은 상대적으로 높다는 것을 의미한다.



#### Cross-Entropy Function

1. $cost=-\sum_{k=1}^ky_ilog(p_i)$

   - $k$는 class(결과)의 개수

   - $y_i$는 실제 One-Hot 벡터의 j번째 원소

   - $p_i$는 샘플 데이터가 j번째 클래스일 확률



2.  $cost=-\sum_{k=1}^ky_ilog(p_i)$를   
   $D(\hat{Y_i},Y_i)$로도 표현 가능함.
3. 만일, N개의 traininig set이 있을 때는  
   $cost=-\frac{1}{N}\sum_{i=1}^{n}\sum_{j=1}^{k}y_j^ilog(p_j^i)$ 
